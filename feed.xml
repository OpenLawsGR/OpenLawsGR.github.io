<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>OpenLaws</title>
    <description>A Project Opening Greek Legislation to Citizens</description>
    <link>http://www.openlaws.grhttp://www.openlaws.gr/</link>
    <atom:link href="http://www.openlaws.grhttp://www.openlaws.gr/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Methodology!</title>
        <description>&lt;p&gt;In this post we present a general layout of the methodology that we are following:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Collection of Greek Legislation.&lt;/strong&gt; Initially, there is the need to collect all greek law documents that are published from the &lt;a href=&quot;http://www.et.gr&quot;&gt;National Printing Service (NPS)&lt;/a&gt; in the Government Gazzette. In a &lt;a href=&quot;http://www.openlaws.gr/legislation/code/2015/06/22/scrapying-greek-laws/&quot;&gt;previous post&lt;/a&gt; we have shown in detail how a crawler that undertakes this task can be implemented.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformation of Files to Plain Text Format.&lt;/strong&gt; Law documents are published as PDF files and as a result there is the need to transform them in a format that can easily be processed. We use the &amp;lsquo;pdftotext&amp;rsquo; open source utility for Linux to transform PDF files to plain text documents. Older PDF files (usually before 2004) contain scanned images and since OCR processing can be erroneous and these errors are detrimental to pattern matching, we decided to &lt;u&gt;exclude these files and limit the application of our system to newest laws&lt;/u&gt;. OCR training is out of the scope of this project, however it is an extension that we are willing to examine in the future.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distinction of Different Legislative Text Types.&lt;/strong&gt; Each issue (PDF file) of the Government Gazzette may contain different types of legislative texts (e.g. laws, presidential decrees, ministerial decisions etc.). As our work currently focuses solely on laws, we need to distinguish laws from other legislative types residing in the same file. This is achievable by identifying the text patterns that flag the existence of a law and with the application of regular expressions one is able to isolate the text of each law.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Law Text Preprocessing.&lt;/strong&gt; Law text preprocessing is an important phase of the analysis cycle as it corrects several errors, either already present in the PDF files or produced during the previous steps. Such a process is expected to facilitate future analysis steps and natural language processing. In this phase, regular expressions can be used to remove irrelevant text that is transferred to the plain text files when transforming the PDF files: PDF page numbering, digital signature text, original file’s headers and footers, characters that are considered as garbage etc. Moreover, words divided by hyphens at the end of lines are fixed by concatenation. Finally, some abbreviations are temporarily substituted by the full words, as our first experimentations with the python suite &lt;a href=&quot;http://www.nltk.org/&quot;&gt;NLTK&lt;/a&gt;, which provides tools for natural language processing, showed that they sometimes cause problems, e.g. resulting in wrong sentence segmentation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Structural Analysis.&lt;/strong&gt; In order to automatically perform transformations to a law&amp;rsquo;s content according to a modification, it is necessary to be aware of the structure of each law. Modifications refer to structural elements (such as articles, paragraphs, sub-paragraphs etc.) that a reader can easily search and identify within the text. However, a machine needs to analyze the text to recognize these elements. We are currently working on a module that parses each law text, performs structural analysis and identifies the structural elements. Laws’ structure follows distinct patterns, in the sense that laws should be written according to rules set by the Central Legislatorial Committee that explicitly define the structural parts of the text. Our module is based on regular expressions that match the patterns that occur according to these rules. After the analysis, we intend to store each law in XML format for internal representation purposed, which allows for a hierarchical layout of nodes, each one corresponding to a structural element.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Identification and Automatic Application of Modifications.&lt;/strong&gt; The last step of the methodology for the implementation of our system involves the identification of modifications in all text files and their recurrent application, in a proper chronological order, on the original law text they refer to, for the consolidation of all law versions. A modification may refer to an addition, suppression or substitution of a text portion. The verbs that are used to declare each case are usually the same or belong to a list of synonyms and can be used to detect possible modifications. Each candidate text portion should get further analyzed in order to detect if it follows a modification’s pattern (e.g. containing references to structural elements of another law) or it happens to appear within the law content as a simple verb occurrence. When a modification is identified and categorized as one of the three cases, a semantic and syntactic analysis will reveal which elements should change (e.g. a specific article of a law) and how (e.g. a specific text should be added at the beginning of that article). When all modifications are identified and analyzed, a script should traverse all laws and transform the appropriate laws’ texts according to the defined actions. The identification of the position of each modification’s referencing elements within the files should be straightforward due to the structural analysis presented in the previous step. When a new version of a law is produced, it will be committed and pushed to the version control repository.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Steps 1-4 are already successfully implemented. Step 5 is almost done and we will soon report on our respective work. Step 6 is still under design and normally we may need to reconsider some of our thoughts.&lt;/p&gt;
</description>
        <pubDate>Sun, 30 Aug 2015 13:03:00 +0300</pubDate>
        <link>http://www.openlaws.grhttp://www.openlaws.gr/legislation/2015/08/30/Methodology/</link>
        <guid isPermaLink="true">http://www.openlaws.grhttp://www.openlaws.gr/legislation/2015/08/30/Methodology/</guid>
      </item>
    
      <item>
        <title>New Conference Publication!</title>
        <description>&lt;p&gt;Our submission entitled &amp;ldquo;Automated Analysis of Greek Legislative Texts for Version Control: Limitations, Caveats and Challenges&amp;rdquo; was accepted as a short paper and will be presented at the &lt;a href=&quot;http://pci2015.teiath.gr&quot;&gt;19th Panhellenic Conference on Informatics (PCI 2015)&lt;/a&gt;, which will take place in Athens, 1-3 October 2015.&lt;/p&gt;

&lt;p&gt;The paper presents our work so far and focuses on the problems that we are facing in this effort:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There are no official APIs or web services available, providing applications with access to law texts.&lt;/li&gt;
&lt;li&gt;The provided law texts are not published in a format directly machine processable. PDF files need to get transformed to other formats (such as plain text) and this process could introduce errors, while the preservation of elements such as tables is difficult to get supported. Consequently, texts are not semantically annotated, while such a practice could take place before a law gets published at a low cost rather than performing the challenging task of automatic annotation at a later stage.&lt;/li&gt;
&lt;li&gt;PDF files before 2004 contain scanned images of the printed laws. Some files also contain scanned images of texts in other languages (e.g. laws approving international agreements). These images are often of low quality, sometimes with skew positioning and contrast variations and their processing with OCR can be an error prone step. &lt;/li&gt;
&lt;li&gt;Some PDF files are problematic. For example, all files between 2000 and 2005 have imperfect encoding information attached and their transformation to plain texts results in wrong characters. An extra step of character mapping for these files solves this problem. Other examples include a PDF file without content (there is only the phrase “No content was attributed to this issue”!) or a PDF file with wrong pattern text before the signature of the President of the Hellenic Republic (“The President of the Parliament” instead of “The President of the Hellenic Republic”).&lt;/li&gt;
&lt;li&gt;In some cases the regular expressions that we use, although correct, are unable to match text patterns. The cause of this problem is the ambiguous mixing of look-alike Greek and Latin characters in the text. For example, the Greek word “ΝΟΜΟΣ” that means law and is used for distinguishing laws from other legislative types is found to contain characters encoded in the Latin character set in all positions (capital letters N, O, M look the same in both alphabets) apart from the last one in several cases.&lt;/li&gt;
&lt;li&gt;Legislative texts before 1982 are written in a different form of the Greek language, called katharevousa that is polytonic (multi accent). As a result, processing and analysis for recent laws written in the current form of the Greek language are not applicable for the case of katharevousa and all effort needs to be repeated.&lt;/li&gt;
&lt;li&gt;Legislative texts should be written according to the instructions that the Central Legislatorial Committee has issued. However, these instructions were only published in 1984, consequently before that date there was not a formally agreed way for the composition of laws, nevertheless it seems that there is a set of fixed phrases that are usually used. In any case, a careful review of legislative texts is needed to reveal all adopted patterns. Moreover, legislative drafting and linguistic rules are quite general and do not cover all possible cases, increasing the complexity of pattern matching. Finally, we have found several cases of laws where these instructions are not followed. For example, according to an instruction all law titles should appear in nominative case; however we have identified laws whose titles appear in genitive case.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 29 Jun 2015 19:37:00 +0300</pubDate>
        <link>http://www.openlaws.grhttp://www.openlaws.gr/publications/2015/06/29/PCI2015/</link>
        <guid isPermaLink="true">http://www.openlaws.grhttp://www.openlaws.gr/publications/2015/06/29/PCI2015/</guid>
      </item>
    
      <item>
        <title>Scrapying Greek Laws from UltraCl@rity</title>
        <description>&lt;p&gt;In this post we present a step by step tutorial describing the procedure we followed to download all PDF documents containing law texts that form Greek legislation.&lt;/p&gt;

&lt;p&gt;Laws in Greece are published by the &lt;a href=&quot;http://www.et.gr&quot;&gt;National Printing Service (NPS)&lt;/a&gt; as PDF files, a format that is, from a technological point of view, unfriendly for their processing. Since 2010 these files are available to download for free. Unfortunately, NPS does not provide any API that would allow software applications to request and grant access to these documents.&lt;/p&gt;

&lt;p&gt;Another service that could be used to obtain these documents is &lt;a href=&quot;https://yperdiavgeia.gr/&quot;&gt;UltraCl@rity&lt;/a&gt;, which is an unofficial and voluntarily developed search engine for public sector decisions. This service apart from decisions published in the governmental &lt;a href=&quot;https://diavgeia.gov.gr/&quot;&gt;Cl@rity&lt;/a&gt; website, contains also the total set of Greek legislation. While it provides an API for querying content published in Cl@rity website, it does not support queries for the total set of Greek laws. &lt;/p&gt;

&lt;p&gt;To deal with this situation, our approach was to implement a web crawler. So, here comes the powerful framework &amp;ldquo;Scrapy&amp;rdquo;.&lt;/p&gt;

&lt;h3 id=&quot;what-is-scrapy?&quot;&gt;What is Scrapy?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;http://scrapy.org/&quot;&gt;Scrapy&lt;/a&gt; is a web crawling framework written in Python. It is a fast and open-source tool for extracting data from websites. Using Scrapy we implemented a crawler that automatically downloads all law PDF files from the UltraCl@rity website. Our preference for this unofficial service instead of the NPS website results from the fact that URLs follow a consistent pattern making it easy to form the required http requests. 
For example:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;ss&quot;&gt;https&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yperdiavgeia&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;laws&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;year_from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;XXXX&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;year_to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;YYYY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;teuxos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;returns an html page with links to all law documents published between year XXXX and year YYYY. Adding an extra &lt;em&gt;page&lt;/em&gt; parameter at the end of the URL allows to easily deal with pagination.&lt;/p&gt;

&lt;p&gt;On the other hand, the NPS website provides a form (available &lt;a href=&quot;http://www.et.gr/index.php/2013-01-28-14-06-23/2013-01-29-08-13-13&quot;&gt;here&lt;/a&gt;) that limits search to the first 200 results, making the overall procedure more complex, plus the fact that pagination is implemented client-side with JavaScript.&lt;/p&gt;

&lt;h3 id=&quot;how-do-we-use-scrapy?&quot;&gt;How do we use Scrapy?&lt;/h3&gt;

&lt;p&gt;To get a feeling of how scrapy works there is an online tutorial plus installation instructions available &lt;a href=&quot;http://doc.scrapy.org/en/0.24/&quot;&gt;here&lt;/a&gt;. In this post we assume that Scrapy is already installed and a new project (named ultraclarity) has been created, following the structure as described in the tutorial. &lt;/p&gt;

&lt;p&gt;Before starting the development of the spider it is necessary to define the data that we want to scrape. This is done by modifying the &lt;em&gt;items.py&lt;/em&gt; file. We create &lt;a href=&quot;http://doc.scrapy.org/en/0.24/topics/items.html&quot;&gt;Items&lt;/a&gt; which are described as containers that will be loaded with the scraped data. Inside items.py file a class named &lt;em&gt;UltraclarityItem&lt;/em&gt; has been automatically created that takes a scrapy item object as a parameter and inside that class we define some atrributes (according to what we need to download from the website) as shown below:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;UltraclarityItem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scrapy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scrapy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scrapy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;desc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scrapy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above model defines three attributes used to capture the &lt;em&gt;title&lt;/em&gt; (in our case this will be used as the name of the file that the data will be saved to), the &lt;em&gt;URL&lt;/em&gt; (where is the item that we want to scrape), and the description referred as &lt;em&gt;desc&lt;/em&gt; (later it will become clear that this should contain the content of the PDF file). &lt;/p&gt;

&lt;p&gt;Next, we have to declare our Spider. Spiders are user-written classes that must be well defined. With the term well-defined we mean that it is important to instantiate some mandatory fields, for example its unique name etc, create the URLs to download and define through methods how content is parsed to extract items. There are some basic methods to use but scrapy also allows to create or override functions in order to obtain a more customized behaviour. All spiders are set up inside a folder named &lt;em&gt;spiders&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In our project we used CrawlSpider which is the most common spider used to crawl regular websites. Inside spiders folder we create a new file named ultraclarityspider.
As we will developing the spider, packages need to be imported to avoid errors when compiling the code. For example to be able to make http requests from scrapy.http we need to import Request. These packages are written in ultraclarityspider and are shown below:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scrapy&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scrapy.spiders&lt;/span&gt;     &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CrawlSpider&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scrapy.selector&lt;/span&gt;    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Selector&lt;/span&gt; 
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ultraclarity.items&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;UltraclarityItem&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scrapy.http&lt;/span&gt;        &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Request&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We are now ready to begin developing the spider. Firstly, we create a class named &lt;em&gt;UltraclaritySpider&lt;/em&gt; which subclasses the CrawlSpider and afterwards we specify its mandatory fields: a &lt;em&gt;name&lt;/em&gt; that we will use to call from the terminal, an optional list named &lt;em&gt;Allowed_domains&lt;/em&gt; which is a list of domains where the spider will begin to crawl from, when no particular URLs are specified and finally a list of URLs where the Spider will begin to crawl from. The code below illustrates the above description.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;UltraclaritySpider&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CrawlSpider&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;ultraclarity&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;allowed_domains&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;yperdiavgeia.gr&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
    &lt;span class=&quot;n&quot;&gt;start_urls&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
       
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;year&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xxxx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yyyy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;start_urls&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;http://yperdiavgeia.gr/laws/search/year_from:&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/year_to:&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/teuxos:A&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In order to define &lt;em&gt;start_urls&lt;/em&gt; we need to know which years of publications of laws we intend to search for. So, we use a &lt;em&gt;for&lt;/em&gt; loop where &lt;em&gt;xxxx&lt;/em&gt; and &lt;em&gt;yyyy&lt;/em&gt; is the range of years to work with. &lt;/p&gt;

&lt;p&gt;Inside &lt;em&gt;UltraclaritySpider&lt;/em&gt; class we also implement three methods to help us scrape data. The first one, &lt;em&gt;parse&lt;/em&gt; is a basic method, a default callback used by Scrapy to process downloaded responses. &lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Selector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_pages&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;//div[@id=&amp;quot;total-results&amp;quot;]/text()&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_pages&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/page:&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;callback&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_objects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code shows that we have used a mechanism called &lt;a href=&quot;http://doc.scrapy.org/en/0.24/topics/selectors.html&quot;&gt;Selector&lt;/a&gt; to select which data we are going to extract. Selectors select certain parts of the HTML document specified either by &lt;a href=&quot;http://www.w3.org/TR/xpath/&quot;&gt;XPath&lt;/a&gt; or CSS expressions. XPath (which we are using) is a language for selecting nodes in XML and HTML documents. In this method we use selectors to deal with pagination. First, we follow the path to the element that contains the total number of results according to our search, then we use a &lt;em&gt;pages&lt;/em&gt; function that divides this number with the number of results per page (UltraCl@rity Service by default uses ten documents per page) and returns the number of pages. As mentioned above we can use an extra &lt;em&gt;page&lt;/em&gt; parameter to construct the urls dynamically. Multiple requests are then processed with this tecnhique and for such requests we implement a callback function named &amp;ldquo;parse_objects&amp;rdquo;. &lt;/p&gt;

&lt;p&gt;Now that we have created all requests for every page, we need to fill the items that we created in UltraclarityItem class. Spiders are expected to return their scraped data inside &lt;em&gt;item&lt;/em&gt; objects and by taking advantage of selectors and following the tree structure of every page we can easily spot the information we need. In our case, documents are stored in divs having class &lt;em&gt;law&lt;/em&gt; or &lt;em&gt;law alt&lt;/em&gt; so for every path we follow we call an instance of the UltraclarityItem and we fill the item title (following the same pattern to all files e.g &lt;em&gt;government-gazzette-issue-num_issue-type_year-of-publication&lt;/em&gt;) and the URL that points to the PDF file. As this URL points directly to the law document, we just need to perform requests based on these URLs, define a callback function named &lt;em&gt;parse_urls&lt;/em&gt; and just pass the item as metadata so that we can use it in the callback function:&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;parse_objects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sel&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Selector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;paths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;//div[@class=&amp;quot;law&amp;quot;] | //div[@class=&amp;quot;law alt&amp;quot;]&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paths&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;UltraClarityItem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;a[@class=&amp;quot;title&amp;quot;]/text()&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;_&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;url&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;a[@class=&amp;quot;title&amp;quot;]/@href&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;url&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;callback&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;item&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The last method of the class is used to store the file in the item&amp;rsquo;s description.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;parse_urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;meta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;item&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;desc&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;body&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, in the last step we need to store every item that contains information locally to our computer. After an item has been scraped by a spider, it is sent to the item pipeline which processes it through several components that are executed sequentially. Scrapy provides a placeholder file when you create a project in &lt;em&gt;project&lt;em&gt;name/project&lt;/em&gt;name/pipelines.py&lt;/em&gt; that one can modify. Our pipeline is a class (&lt;em&gt;UltraclarityPipeline&lt;/em&gt;) that implements a simple method named &lt;em&gt;process_item&lt;/em&gt; which stores every item in a file based on the item&amp;rsquo;s title. All we need to do is just open files and store in them the text. For better organisation, we store files in folders based on their year of publication.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ultraclarity.items&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;UltraclarityItem&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;UltraclarityPipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;process_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spider&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;laws/&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;makedirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;laws/&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
        
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;laws/&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;_&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;/&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;desc&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is it! Just calling our spider&amp;rsquo;s name from the terminal will do the job!&lt;/p&gt;
</description>
        <pubDate>Mon, 22 Jun 2015 03:37:19 +0300</pubDate>
        <link>http://www.openlaws.grhttp://www.openlaws.gr/legislation/code/2015/06/22/scrapying-greek-laws/</link>
        <guid isPermaLink="true">http://www.openlaws.grhttp://www.openlaws.gr/legislation/code/2015/06/22/scrapying-greek-laws/</guid>
      </item>
    
      <item>
        <title>Short Anatomy of a Greek Law</title>
        <description>&lt;p&gt;Greek laws are published by the &lt;a href=&quot;http://www.et.gr&quot;&gt;National Printing Service&lt;/a&gt; as texts contained in issues of the Journal of the Greek Government. These issues are actually pdf files.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://www.openlaws.gr/images/lawgr.png&quot; alt=&quot;Greek law texts example&quot;&gt;&lt;/p&gt;

&lt;p&gt;Law texts within an issue contain references to parts (e.g. articles, paragraphs etc.) of other issues. The reader should manage these references by himself as there are no links leading to the referenced content: search and download the appropriate pdf files and scan them to locate the content.&lt;/p&gt;

&lt;p&gt;Law modifications (additions, deletions, insertions, updates) are verbally described. For example consider the following phrase from a Greek law:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Article 19 of law 3429/2005 (issue 314 A) is substituted as follows: &amp;ldquo;&amp;hellip;&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In such a case, the reader should find the text of law 3429/2005, locate article 19 and substitute its content with the new one. Usually, such modifications of a law&amp;rsquo;s text reside in many different issues, while it is possible that the same part of a law is modified several times through its lifecycle&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Jun 2015 19:55:00 +0300</pubDate>
        <link>http://www.openlaws.grhttp://www.openlaws.gr/legislation/2015/06/17/anatomy-of-greek-law/</link>
        <guid isPermaLink="true">http://www.openlaws.grhttp://www.openlaws.gr/legislation/2015/06/17/anatomy-of-greek-law/</guid>
      </item>
    
      <item>
        <title>Hello World!</title>
        <description>&lt;p&gt;As in most countries, in Greece laws are published as documents usually containing many references to other legal documents and often verbally describing modifications to existing laws. Assembling all pieces in order to find the valid version of a law at a given moment is a tedious task, requiring increased cognitive load. Commercial databases that codify legislation are available, however there is not such a free service for citizens, not even for public bodies. Sometimes, one may find codified laws in public organizations or other bodies (e.g. communities) websites, however their update with the last changes depends on each website manager.&lt;/p&gt;

&lt;p&gt;As legal language usually follows quite formal patterns, natural language processing techniques can be used for the automatic consolidation of Greek law documents and their publishing through version control systems, which allow the tracking of different text versions. &lt;/p&gt;

&lt;h3 id=&quot;our-effort&quot;&gt;Our effort&lt;/h3&gt;

&lt;p&gt;We will use this blog to post updates on our work towards the above described direction. We are trying to create software that will automatically identify law modifications and apply them on the original law texts. Upon success, we will use Github for law version tracking.&lt;/p&gt;

&lt;p&gt;Stay tuned!&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Jun 2015 03:37:19 +0300</pubDate>
        <link>http://www.openlaws.grhttp://www.openlaws.gr/general/2015/06/17/hello-world/</link>
        <guid isPermaLink="true">http://www.openlaws.grhttp://www.openlaws.gr/general/2015/06/17/hello-world/</guid>
      </item>
    
  </channel>
</rss>
